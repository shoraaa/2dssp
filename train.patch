*** a/train.py
--- b/train.py
@@
 def rollout_episode(
     model: NeuralSolver,
     env: TilePlacementEnv,
     tile_embs: torch.Tensor,
     device,
     temperature: float,
     max_steps: int,
     overlap_bonus_coef: float = 0.0,
+    amp_enabled: bool = False,
 ) -> Tuple[torch.Tensor, torch.Tensor, float, int, float]:
@@
-    model.eval()
+    # keep model in train mode for proper policy sampling regularization
+    model.train()
     env.reset()
     logps: List[torch.Tensor] = []
-    rewards: List[float] = []
-    entropy_sum = 0.0
+    rewards: List[float] = []
+    entropy_sum_t = torch.zeros((), device=device, dtype=torch.float32)
     steps = 0
 
     while not env.done and steps < max_steps:
         sb = build_step_batch_from_env(env, tile_embs, device=device)
-        if not sb.cand_mask.any().item():
+        # Avoid device sync on .item()
+        if not bool(sb.cand_mask.any().detach().cpu()):
             break
 
-        logits, _ = model(
-            sb.occ,
-            sb.tiles_left,
-            sb.tiles_left_mask,
-            sb.cand_feats,
-            sb.cand_mask,
-            sb.cand_tile_idx,
-        )
+        with autocast(enabled=amp_enabled):
+            logits, _ = model(
+                sb.occ,
+                sb.tiles_left,
+                sb.tiles_left_mask,
+                sb.cand_feats,
+                sb.cand_mask,
+                sb.cand_tile_idx,
+            )
         masked_logits = logits / max(1e-6, temperature)
         masked_logits = masked_logits.masked_fill(~sb.cand_mask, -1e9)
         logp_all = F.log_softmax(masked_logits, dim=-1).squeeze(0)
         p_all = logp_all.exp()
 
-        ent = -(p_all * logp_all).sum()
-        entropy_sum += float(ent.item())
+        entropy_sum_t = entropy_sum_t - (p_all * logp_all).sum()
 
         probs = p_all.clone()
         probs[~sb.cand_mask.squeeze(0)] = 0.0
         probs = probs / (probs.sum() + 1e-9)
-        a = int(torch.distributions.Categorical(probs).sample().item())
-        logps.append(logp_all[a])
+        a_t = torch.distributions.Categorical(probs).sample()  # tensor on device
+        logps.append(logp_all[a_t])
 
-        old_m, _ = layout_bbox(env.placements, env.n)
         raw_cands = env.generate_candidates()
-        env.step(raw_cands[a])
+        a = int(a_t)  # single cheap scalar sync
+        # step and get reward bits without scanning layout
+        _, delta_m, H_sum = env.step_and_metrics(raw_cands[a])
-        new_m, _ = layout_bbox(env.placements, env.n)
-        delta_m = old_m - new_m
-
-        H_sum = sb.cand_feats[0, a, 0].item()
-        r = float(delta_m) + overlap_bonus_coef * float(H_sum)
+        # reward: positive if bbox decreased (delta_m is increase; use -delta_m)
+        r = float(-delta_m) + overlap_bonus_coef * float(H_sum)
         rewards.append(r)
         steps += 1
 
-    final_m, _ = layout_bbox(env.placements, env.n)
+    # compute final m from bbox directly (no scan)
+    xmin, xmax, ymin, ymax = env.bbox
+    final_m = float(max(xmax - xmin + 1, ymax - ymin + 1))
-    return (torch.stack(logps) if logps else torch.empty(0, device=device),
-            torch.tensor(rewards, dtype=torch.float32),
-            float(final_m),
-            steps,
-            float(entropy_sum))
+    return (torch.stack(logps) if logps else torch.empty(0, device=device),
+            torch.tensor(rewards, dtype=torch.float32),
+            float(final_m),
+            steps,
+            float(entropy_sum_t.detach().cpu()))
@@
 def evaluate_instance(model: NeuralSolver, env: TilePlacementEnv, tile_embs: torch.Tensor, device, temperature=0.7, greedy=True):
@@
-    while not env.done and steps < 10000:
+    while not env.done and steps < 10000:
         sb = build_step_batch_from_env(env, tile_embs, device=dev)
-        if not sb.cand_mask.any().item():
+        if not bool(sb.cand_mask.any().detach().cpu()):
             break
         a = model_pick_action(model, sb, temperature=temperature, argmax=greedy)
         raw_cands = env.generate_candidates()
-        env.step(raw_cands[a])
+        env.step(raw_cands[a])  # metrics not needed in greedy eval path
         steps += 1
-    m, _ = layout_bbox(env.placements, env.n)
-    return m, steps
+    xmin, xmax, ymin, ymax = env.bbox
+    m = max(xmax - xmin + 1, ymax - ymin + 1)
+    return float(m), steps
@@
     for ep in tbar:
@@
-            logps, rewards, final_m, steps, ent_sum = rollout_episode(
+            logps, rewards, final_m, steps, ent_sum = rollout_episode(
                 model=model,
                 env=env,
                 tile_embs=tile_embs,
                 device=device,
                 temperature=temperature,
                 max_steps=max_steps_per_episode,
                 overlap_bonus_coef=overlap_bonus_coef,
+                amp_enabled=scaler.is_enabled(),
             )
@@
-        entropy_term = torch.as_tensor(total_entropy_sum, dtype=torch.float32, device=device)
-        loss = policy_loss_total - entropy_coef * entropy_term
+        entropy_term = torch.as_tensor(total_entropy_sum, dtype=torch.float32, device=device)
+        loss = policy_loss_total - entropy_coef * entropy_term
